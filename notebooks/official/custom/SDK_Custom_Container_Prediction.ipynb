{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TBourtonBumble/dummy_model_generate/blob/main/notebooks/official/custom/SDK_Custom_Container_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Deploying Iris-detection model using FastAPI and Vertex AI custom container serving\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/SDK_Custom_Container_Prediction.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fcustom%2FSDK_Custom_Container_Prediction.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/custom/SDK_Custom_Container_Prediction.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/SDK_Custom_Container_Prediction.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this tutorial, you build a scikit-learn model and deploy it on Vertex AI using the custom container method. You use the FastAPI Python web server framework to create a prediction endpoint. You also incorporate a preprocessor from training pipeline into your online serving application.\n",
        "\n",
        "Learn more about [Custom training](https://cloud.google.com/vertex-ai/docs/training/custom-training) and [Vertex AI Prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-predictions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbd99f7bfc8e"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to create, deploy and serve a custom classification model on Vertex AI. This notebook focuses more on deploying the model than on the design of the model itself.\n",
        "\n",
        "\n",
        "This tutorial uses the following Vertex AI services and resources:\n",
        "\n",
        "- Vertex AI models\n",
        "- Vertex AI endpoints\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Train a model that uses flower's measurements as input to predict the class of iris.\n",
        "- Save the model and its serialized pre-processor.\n",
        "- Build a FastAPI server to handle predictions and health checks.\n",
        "- Build a custom container with model artifacts.\n",
        "- Upload and deploy custom container to Vertex AI Endpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fe0bb78c9ce"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This tutorial uses R.A. Fisher's Iris dataset, a small and popular dataset for machine learning experiments. Each instance has four numerical features, which are different measurements of a flower, and a target label that\n",
        "categorizes the flower into: **Iris setosa**, **Iris versicolour** and **Iris virginica**.\n",
        "\n",
        "This tutorial uses [a version of the Iris dataset available in the\n",
        "scikit-learn library](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c681f532cf64"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "* Artifact Registry\n",
        "* Cloud Build\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), [Artifact Registry pricing](https://cloud.google.com/artifact-registry/pricing) and [Cloud Build pricing](https://cloud.google.com/build/pricing) and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0316df526f8"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9065e8d7f0fb"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n",
        "\n",
        "Write the requirements needed for building container into a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "747f59abb3a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b02d6ac1-16d1-4346-8c87-9fb8007d13e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "joblib~=1.5.3\n",
        "numpy<2\n",
        "scikit-learn~=1.6.1\n",
        "google-cloud-storage~=3.9.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "479caccfb277"
      },
      "source": [
        "Install the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AYFxyhn8JJZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install -U  uvicorn\n"
      ],
      "metadata": {
        "id": "x7HBglkZnytZ",
        "outputId": "fd7bfcc2-8934-471b-b869-6d43dac03227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.40.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1fd00fa70a2a"
      },
      "outputs": [],
      "source": [
        "# Required in Docker serving container\n",
        "! pip3 install -U  -r requirements.txt -q\n",
        "\n",
        "# For local FastAPI development and running\n",
        "! pip3 install -U  \"uvicorn[standard]~=0.40.0\" fastapi~=0.63 -q\n",
        "\n",
        "# Vertex SDK for Python\n",
        "! pip3 install --upgrade --quiet  google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "D-ZBOjErv5mM"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d8e61720c2a"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "429a9f6d237d"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0c74b83ece6c"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.__version__"
      ],
      "metadata": {
        "id": "NI8lcXbUlns6",
        "outputId": "3bf4323d-39fb-4f2d-d0b4-14dc72daaefb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.26.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfEglUHQk9S3"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"notebooks-platform-rd\"  # @param {type:\"string\"}\n",
        "LOCATION = \"europe-west4\"  # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://test-model-artifact-notebooks-rd\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT1acPShmb5r",
        "outputId": "79d08a1f-00f4-4a2e-e18c-613050052ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://test-model-artifact-notebooks-rd/...\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.storage.buckets.create) HTTPError 409: Your previous request to create the named bucket succeeded and you already own it.\n"
          ]
        }
      ],
      "source": [
        "! gcloud storage buckets create --location={LOCATION} --project={PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3330b4f12a0d"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e088ea8cd4a0"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3938f6d37a1"
      },
      "source": [
        "### Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e95ca1e5e07c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79b1bc1a-e04d-4489-dc1f-31db4109171d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.26.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Configure resource names\n",
        "\n",
        "Set a name for the following parameters:\n",
        "\n",
        "`MODEL_ARTIFACT_DIR` - Folder directory path to your model artifacts within a Cloud Storage bucket, for example: \"my-models/fraud-detection/trial-4\"\n",
        "\n",
        "`REPOSITORY` - Name of the Artifact Repository to create or use.\n",
        "\n",
        "`IMAGE` - Name of the container image that is pushed to the repository.\n",
        "\n",
        "`MODEL_DISPLAY_NAME` - Display name of Vertex AI model resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "MODEL_ARTIFACT_DIR = \"[your-artifact-directory]\"  # @param {type:\"string\"}\n",
        "REPOSITORY = \"[your-repository-name]\"  # @param {type:\"string\"}\n",
        "IMAGE = \"[your-image-name]\"  # @param {type:\"string\"}\n",
        "MODEL_DISPLAY_NAME = \"[your-model-display-name]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the defaults if no names were specified\n",
        "if MODEL_ARTIFACT_DIR == \"[your-artifact-directory]\":\n",
        "    MODEL_ARTIFACT_DIR = \"custom-container-prediction-model\"\n",
        "\n",
        "if REPOSITORY == \"[your-repository-name]\":\n",
        "    REPOSITORY = \"custom-container-prediction\"\n",
        "\n",
        "if IMAGE == \"[your-image-name]\":\n",
        "    IMAGE = \"sklearn-fastapi-server\"\n",
        "\n",
        "if MODEL_DISPLAY_NAME == \"[your-model-display-name]\":\n",
        "    MODEL_DISPLAY_NAME = \"sklearn-custom-container\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c2d091d9e73"
      },
      "source": [
        "## Write your pre-processor\n",
        "Standardize the training data so that each numerical feature column has a mean of 0 and a standard deviation of 1 [can improve your model](https://developers.google.com/machine-learning/crash-course/representation/cleaning-data).\n",
        "\n",
        "Define an `app` folder and create `preprocess.py`, which contains a class to perform standardization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6e74556ea0b4"
      },
      "outputs": [],
      "source": [
        "%mkdir app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "58d843d21fa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e543eb7-c114-4be7-ff57-6a38a70ed538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/preprocess.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/preprocess.py\n",
        "import numpy as np\n",
        "\n",
        "class MySimpleScaler(object):\n",
        "    def __init__(self):\n",
        "        self._means = None\n",
        "        self._stds = None\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        if self._means is None:  # during training only\n",
        "            self._means = np.mean(data, axis=0)\n",
        "\n",
        "        if self._stds is None:  # during training only\n",
        "            self._stds = np.std(data, axis=0)\n",
        "            if not self._stds.all():\n",
        "                raise ValueError(\"At least one column has standard deviation of 0.\")\n",
        "\n",
        "        return (data - self._means) / self._stds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b816cd52f4b"
      },
      "source": [
        "## Train and store model with pre-processor\n",
        "Use `preprocess.MySimpleScaler` to preprocess the iris data, and then train a model using scikit-learn.\n",
        "\n",
        "After training, export your trained model as a joblib (`.joblib`) file and export your `MySimpleScaler` instance as a pickle (`.pkl`) file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "43e47249f736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac68c95-a18f-4874-af2f-13f24bef375d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/app\n"
          ]
        }
      ],
      "source": [
        "%cd app/\n",
        "\n",
        "import pickle\n",
        "\n",
        "import joblib\n",
        "from preprocess import MySimpleScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "scaler = MySimpleScaler()\n",
        "\n",
        "X = scaler.preprocess(iris.data)\n",
        "y = iris.target\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X, y)\n",
        "\n",
        "joblib.dump(model, \"model.joblib\")\n",
        "with open(\"preprocessor.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3849066a33bd"
      },
      "source": [
        "### Upload model artifacts and custom code to Cloud Storage\n",
        "\n",
        "Before you can deploy your model for serving, Vertex AI needs access to the following files in Cloud Storage:\n",
        "\n",
        "* `model.joblib` (model artifact)\n",
        "* `preprocessor.pkl` (model artifact)\n",
        "\n",
        "Run the following commands to upload your files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ca67ee52d4d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499edbb3-2ef3-41fb-ccfc-354256450a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://model.joblib to gs://test-model-artifact-notebooks-rd/custom-container-prediction-model/model.joblib\n",
            "Copying file://preprocessor.pkl to gs://test-model-artifact-notebooks-rd/custom-container-prediction-model/preprocessor.pkl\n",
            "\n",
            "Average throughput: 183.2kiB/s\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!gcloud storage cp model.joblib preprocessor.pkl {BUCKET_URI}/{MODEL_ARTIFACT_DIR}/\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480a1d88ecdb"
      },
      "source": [
        "## Build a FastAPI server\n",
        "\n",
        "To serve predictions from the classification model, build a FastAPI server application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "94af0ba5eadd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9e99be-e08d-4d4c-8c39-813a0defcb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/main.py\n",
        "from fastapi import FastAPI, Request\n",
        "\n",
        "import joblib\n",
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from google.cloud import storage\n",
        "from preprocess import MySimpleScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "gcs_client = storage.Client()\n",
        "\n",
        "with open(\"preprocessor.pkl\", 'wb') as preprocessor_f, open(\"model.joblib\", 'wb') as model_f:\n",
        "    gcs_client.download_blob_to_file(\n",
        "        f\"{os.environ['AIP_STORAGE_URI']}/preprocessor.pkl\", preprocessor_f\n",
        "    )\n",
        "    gcs_client.download_blob_to_file(\n",
        "        f\"{os.environ['AIP_STORAGE_URI']}/model.joblib\", model_f\n",
        "    )\n",
        "\n",
        "with open(\"preprocessor.pkl\", \"rb\") as f:\n",
        "    preprocessor = pickle.load(f)\n",
        "\n",
        "_class_names = load_iris().target_names\n",
        "_model = joblib.load(\"model.joblib\")\n",
        "_preprocessor = preprocessor\n",
        "\n",
        "\n",
        "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
        "def health():\n",
        "    return {}\n",
        "\n",
        "\n",
        "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
        "async def predict(request: Request):\n",
        "    body = await request.json()\n",
        "\n",
        "    instances = body[\"instances\"]\n",
        "    inputs = np.asarray(instances)\n",
        "    preprocessed_inputs = _preprocessor.preprocess(inputs)\n",
        "    outputs = _model.predict(preprocessed_inputs)\n",
        "\n",
        "    return {\"predictions\": [_class_names[class_num] for class_num in outputs]}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xJGiPcy_xPQZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "PoVmbylQHist"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "469f55daf250"
      },
      "source": [
        "### Add pre-start script\n",
        "FastAPI executes the following script before starting up the server. Set the environment variable `PORT` to `AIP_HTTP_PORT` for running the FastAPI server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "69f438aca35b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2740f5d-15b4-49f1-a13d-1f72e0261916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/prestart.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/prestart.sh\n",
        "#!/bin/bash\n",
        "export PORT=$AIP_HTTP_PORT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b62ddf1def3"
      },
      "source": [
        "### Create test instances\n",
        "To learn more about formatting input instances in JSON, [read the documentation.](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#request-body-details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b6605e9e6186",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8398b29e-8d06-4bf8-b6fd-e365bd12a3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing instances.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile instances.json\n",
        "{\n",
        "    \"instances\": [\n",
        "        [6.7, 3.1, 4.7, 1.5],\n",
        "        [4.6, 3.1, 1.5, 0.2]\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51e149fdec1b"
      },
      "source": [
        "## Push the container image to Artifact Registry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "240578ec9efe"
      },
      "source": [
        "Write the `Dockerfile`, using `tiangolo/uvicorn-gunicorn-fastapi` as a base image. This automatically runs FastAPI for you using Gunicorn and Uvicorn. Visit [the FastAPI docs about deploying with Docker](https://fastapi.tiangolo.com/deployment/docker/) to learn more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3d3a6b9ed22b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "781d45c2-49ac-44cf-ddb3-3b921444849a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9\n",
        "\n",
        "COPY ./app /app\n",
        "COPY requirements.txt requirements.txt\n",
        "\n",
        "RUN pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .dockerignore\n",
        "\n",
        "drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59HPqhi-G2DE",
        "outputId": "45c555b3-3687-454b-882f-1fe175c2de20"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing .dockerignore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cae10684d2c"
      },
      "source": [
        "### Test the image locally (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04c988201499"
      },
      "source": [
        "#### Build the image locally (optional)\n",
        "\n",
        "Build the image using docker to test it locally.\n",
        "\n",
        "**Note:** In this tutorial, docker is only being used to test the container locally. For deployment to Artifact Registry, Cloud-Build is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "f1e7d639b9cc"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if not IS_COLAB and not os.getenv(\"IS_TESTING\"):\n",
        "    ! sudo docker build \\\n",
        "            --tag=\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\" \\\n",
        "            ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147a555f6c93"
      },
      "source": [
        "#### Run and test the container locally (optional)\n",
        "\n",
        "Test running the container locally in detached mode and provide the environment variables that the container requires. These variables are provided to the container by Vertex AI once deployed. Test the `/health` and `/predict` routes and then stop the running image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "62ed2d334d0f"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB and not os.getenv(\"IS_TESTING\"):\n",
        "    ! sudo docker stop local-iris\n",
        "    ! sudo docker rm local-iris\n",
        "    ! docker run -d -p 80:8080 \\\n",
        "            --name=local-iris \\\n",
        "            -e AIP_HTTP_PORT=8080 \\\n",
        "            -e AIP_HEALTH_ROUTE=/health \\\n",
        "            -e AIP_PREDICT_ROUTE=/predict \\\n",
        "            -e AIP_STORAGE_URI={BUCKET_URI}/{MODEL_ARTIFACT_DIR} \\\n",
        "            \"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "248f481e8e90"
      },
      "source": [
        "Ping the health route."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ce629eea32fd"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB and not os.getenv(\"IS_TESTING\"):\n",
        "    ! curl localhost/health"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d6821fb2b7d"
      },
      "source": [
        "Pass the `instances.json` and test the predict route."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "56986f93438e"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB and not os.getenv(\"IS_TESTING\"):\n",
        "    ! curl -X POST \\\n",
        "      -d @instances.json \\\n",
        "      -H \"Content-Type: application/json; charset=utf-8\" \\\n",
        "      localhost/predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1d6ee697180"
      },
      "source": [
        "Stop and delete the container locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "a29fcbbe0188"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB and not os.getenv(\"IS_TESTING\"):\n",
        "    ! sudo docker stop local-iris\n",
        "    ! sudo docker rm local-iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93002a20a2a6"
      },
      "source": [
        "### Create a repository in Artifact Registry\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d242773d707"
      },
      "source": [
        "#### Enable Artifact Registry API\n",
        "You must enable the Artifact Registry API service for your project.\n",
        "\n",
        "<a href=\"https://cloud.google.com/artifact-registry/docs/enable-service\">Learn more about Enabling service</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9d72f89cabd5"
      },
      "outputs": [],
      "source": [
        "! gcloud --project $PROJECT_ID services enable artifactregistry.googleapis.com\n",
        "\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    ! sudo apt-get update --yes && sudo apt-get --only-upgrade --yes install google-cloud-sdk-cloud-run-proxy google-cloud-sdk-harbourbridge google-cloud-sdk-cbt google-cloud-sdk-gke-gcloud-auth-plugin google-cloud-sdk-kpt google-cloud-sdk-local-extract google-cloud-sdk-minikube google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-app-engine-python google-cloud-sdk-spanner-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-nomos google-cloud-sdk-package-go-module google-cloud-sdk-firestore-emulator kubectl google-cloud-sdk-datastore-emulator google-cloud-sdk-app-engine-python-extras google-cloud-sdk-cloud-build-local google-cloud-sdk-kubectl-oidc google-cloud-sdk-anthos-auth google-cloud-sdk-app-engine-grpc google-cloud-sdk-pubsub-emulator google-cloud-sdk-datalab google-cloud-sdk-skaffold google-cloud-sdk google-cloud-sdk-terraform-tools google-cloud-sdk-config-connector\n",
        "    ! gcloud components update --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23f3ba8346f4"
      },
      "source": [
        "#### Create a docker repository in Artifact Registry\n",
        "Your first step is to create your own Docker repository in Google Artifact Registry.\n",
        "\n",
        "1 - Run the `gcloud artifacts repositories create` command to create a new Docker repository with your region and the description as \"docker repository\".\n",
        "\n",
        "2 - Run the `gcloud artifacts repositories list` command to verify that your repository was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "64fee26c2dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f249ed-318b-43f2-ae8c-cb941e47329f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n",
            "Listing items under project notebooks-platform-rd, across all locations.\n",
            "\n",
            "                                                                                                                          ARTIFACT_REGISTRY\n",
            "REPOSITORY             FORMAT  MODE                 DESCRIPTION                                                                                 LOCATION      LABELS                          ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
            "ml-images              DOCKER  STANDARD_REPOSITORY                                                                                              europe-west4                                  Google-managed key  2026-02-12T12:57:11  2026-02-13T12:48:29  4720.844\n",
            "my-docker-repo-unique  DOCKER  STANDARD_REPOSITORY  Docker repository                                                                           europe-west4                                  Google-managed key  2026-02-12T16:55:45  2026-02-13T08:38:12  583.577\n",
            "streamlit-apps         DOCKER  STANDARD_REPOSITORY  Docker images for Streamlit apps                                                            europe-west4                                  Google-managed key  2026-01-08T05:42:54  2026-02-05T15:20:04  23035.218\n",
            "vertex-workbench-test  DOCKER  STANDARD_REPOSITORY  A test repository for Workbench images                                                      europe-west4                                  Google-managed key  2025-06-10T08:06:30  2026-01-29T16:46:10  8955.651\n",
            "gcr.io                 DOCKER  STANDARD_REPOSITORY                                                                                              us                                            Google-managed key  2025-12-01T09:32:29  2025-12-04T17:11:35  298.522\n",
            "gcf-artifacts          DOCKER  STANDARD_REPOSITORY  This repository is created and used by Cloud Functions for storing function docker images.  us-central1   goog-managed-by=cloudfunctions  Google-managed key  2026-02-09T02:13:06  2026-02-09T03:10:09  48.569\n",
            "ml-images              DOCKER  STANDARD_REPOSITORY  ML pipeline Docker images migrated from dockerio.badoo.com                                  us-central1                                   Google-managed key  2026-02-08T16:28:54  2026-02-08T19:44:52  4720.605\n",
            "python-packages        PYTHON  STANDARD_REPOSITORY  Internal Python packages migrated from pypi.ds.bumble.dev                                   us-central1                                   Google-managed key  2026-02-08T16:29:11  2026-02-08T19:31:03  0.724\n"
          ]
        }
      ],
      "source": [
        "REPOSITORY = \"my-docker-repo-unique\"\n",
        "\n",
        "! gcloud --project $PROJECT_ID artifacts repositories create {REPOSITORY} --repository-format=docker --location={LOCATION} --description=\"Docker repository\"\n",
        "\n",
        "! gcloud --project $PROJECT_ID artifacts repositories list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2e0b8b700aa"
      },
      "source": [
        "### Submit the image\n",
        "Push the image to the created artifact repository using Cloud build.\n",
        "\n",
        "**Note:** The following command automatically considers the Dockerfile from the directory it's being run from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1dd7448f4703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e874187a-c129-4cfe-d51f-11ae8be45a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".   app      Dockerfile     instances.json    sample_data\n",
            "..  .config  .dockerignore  requirements.txt\n",
            "\n",
            "drive\n",
            "Creating temporary archive of 42 file(s) totalling 54.6 MiB before compression.\n",
            "Uploading tarball of [.] to [gs://notebooks-platform-rd_cloudbuild/source/1770997916.365068-bed6df7c093b497db5c893d73520d48c.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/notebooks-platform-rd/locations/europe-west4/builds/cdf2a85b-7435-4751-a788-d9ee3c1efb56].\n",
            "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=europe-west4/cdf2a85b-7435-4751-a788-d9ee3c1efb56?project=924044504931 ].\n",
            "Waiting for build to complete. Polling interval: 1 second(s).\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"cdf2a85b-7435-4751-a788-d9ee3c1efb56\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://notebooks-platform-rd_cloudbuild/source/1770997916.365068-bed6df7c093b497db5c893d73520d48c.tgz#1770997928587731\n",
            "Copying gs://notebooks-platform-rd_cloudbuild/source/1770997916.365068-bed6df7c093b497db5c893d73520d48c.tgz#1770997928587731...\n",
            "\\\n",
            "Operation completed over 1 objects/6.5 MiB.\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/gcb-internal\n",
            "Sending build context to Docker daemon  57.25MB\n",
            "Step 1/4 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9\n",
            "python3.9: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
            "795dbedde24d: Pulling fs layer\n",
            "89d573bf42b3: Pulling fs layer\n",
            "26dfe2fac1c4: Pulling fs layer\n",
            "79d5bd8a8d26: Pulling fs layer\n",
            "081ccf923272: Pulling fs layer\n",
            "c9723aa529b0: Pulling fs layer\n",
            "91c91c91f1d2: Pulling fs layer\n",
            "490d5e44eb35: Pulling fs layer\n",
            "042cad848765: Pulling fs layer\n",
            "1cc558513803: Pulling fs layer\n",
            "029545be4d67: Pulling fs layer\n",
            "8bb416ea4a2e: Pulling fs layer\n",
            "2b33f5fd69e4: Pulling fs layer\n",
            "e6ef226084fd: Pulling fs layer\n",
            "493aa75709ca: Pulling fs layer\n",
            "4f4fb700ef54: Pulling fs layer\n",
            "1d5c1f9d342c: Pulling fs layer\n",
            "687b594511b3: Pulling fs layer\n",
            "e7efa1fefee4: Pulling fs layer\n",
            "042cad848765: Waiting\n",
            "1cc558513803: Waiting\n",
            "029545be4d67: Waiting\n",
            "8bb416ea4a2e: Waiting\n",
            "2b33f5fd69e4: Waiting\n",
            "e6ef226084fd: Waiting\n",
            "493aa75709ca: Waiting\n",
            "4f4fb700ef54: Waiting\n",
            "1d5c1f9d342c: Waiting\n",
            "687b594511b3: Waiting\n",
            "e7efa1fefee4: Waiting\n",
            "91c91c91f1d2: Waiting\n",
            "490d5e44eb35: Waiting\n",
            "081ccf923272: Verifying Checksum\n",
            "081ccf923272: Download complete\n",
            "91c91c91f1d2: Verifying Checksum\n",
            "91c91c91f1d2: Download complete\n",
            "c9723aa529b0: Verifying Checksum\n",
            "c9723aa529b0: Download complete\n",
            "89d573bf42b3: Verifying Checksum\n",
            "89d573bf42b3: Download complete\n",
            "490d5e44eb35: Download complete\n",
            "1cc558513803: Verifying Checksum\n",
            "1cc558513803: Download complete\n",
            "029545be4d67: Verifying Checksum\n",
            "029545be4d67: Download complete\n",
            "795dbedde24d: Verifying Checksum\n",
            "795dbedde24d: Download complete\n",
            "8bb416ea4a2e: Verifying Checksum\n",
            "8bb416ea4a2e: Download complete\n",
            "26dfe2fac1c4: Verifying Checksum\n",
            "26dfe2fac1c4: Download complete\n",
            "042cad848765: Verifying Checksum\n",
            "042cad848765: Download complete\n",
            "e6ef226084fd: Verifying Checksum\n",
            "e6ef226084fd: Download complete\n",
            "4f4fb700ef54: Verifying Checksum\n",
            "4f4fb700ef54: Download complete\n",
            "2b33f5fd69e4: Verifying Checksum\n",
            "2b33f5fd69e4: Download complete\n",
            "493aa75709ca: Verifying Checksum\n",
            "493aa75709ca: Download complete\n",
            "1d5c1f9d342c: Verifying Checksum\n",
            "1d5c1f9d342c: Download complete\n",
            "e7efa1fefee4: Verifying Checksum\n",
            "e7efa1fefee4: Download complete\n",
            "687b594511b3: Verifying Checksum\n",
            "687b594511b3: Download complete\n",
            "79d5bd8a8d26: Verifying Checksum\n",
            "79d5bd8a8d26: Download complete\n",
            "795dbedde24d: Pull complete\n",
            "89d573bf42b3: Pull complete\n",
            "26dfe2fac1c4: Pull complete\n",
            "79d5bd8a8d26: Pull complete\n",
            "081ccf923272: Pull complete\n",
            "c9723aa529b0: Pull complete\n",
            "91c91c91f1d2: Pull complete\n",
            "490d5e44eb35: Pull complete\n",
            "042cad848765: Pull complete\n",
            "1cc558513803: Pull complete\n",
            "029545be4d67: Pull complete\n",
            "8bb416ea4a2e: Pull complete\n",
            "2b33f5fd69e4: Pull complete\n",
            "e6ef226084fd: Pull complete\n",
            "493aa75709ca: Pull complete\n",
            "4f4fb700ef54: Pull complete\n",
            "1d5c1f9d342c: Pull complete\n",
            "687b594511b3: Pull complete\n",
            "e7efa1fefee4: Pull complete\n",
            "Digest: sha256:447693150f3b9224a1dc52218cf536f6065f918f3518f0c135a63f71303da260\n",
            "Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.9\n",
            " ---> aaa48e92235e\n",
            "Step 2/4 : COPY ./app /app\n",
            " ---> 666da8bf0702\n",
            "Step 3/4 : COPY requirements.txt requirements.txt\n",
            " ---> 4b7ec6ffa2e4\n",
            "Step 4/4 : RUN pip install -r requirements.txt\n",
            " ---> Running in fc35702771e1\n",
            "Collecting joblib~=1.5.3\n",
            "  Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 309.1/309.1 kB 15.6 MB/s eta 0:00:00\n",
            "Collecting numpy<2\n",
            "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 72.9 MB/s eta 0:00:00\n",
            "Collecting scikit-learn~=1.6.1\n",
            "  Downloading scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/13.5 MB 85.2 MB/s eta 0:00:00\n",
            "Collecting google-cloud-storage~=3.9.0\n",
            "  Downloading google_cloud_storage-3.9.0-py3-none-any.whl (321 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 321.3/321.3 kB 41.6 MB/s eta 0:00:00\n",
            "Collecting scipy>=1.6.0\n",
            "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.6/38.6 MB 30.7 MB/s eta 0:00:00\n",
            "Collecting threadpoolctl>=3.1.0\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Collecting requests<3.0.0,>=2.22.0\n",
            "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.7/64.7 kB 11.4 MB/s eta 0:00:00\n",
            "Collecting google-resumable-media<3.0.0,>=2.7.2\n",
            "  Downloading google_resumable_media-2.8.0-py3-none-any.whl (81 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.3/81.3 kB 14.9 MB/s eta 0:00:00\n",
            "Collecting google-api-core<3.0.0,>=2.27.0\n",
            "  Downloading google_api_core-2.29.0-py3-none-any.whl (173 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.9/173.9 kB 32.1 MB/s eta 0:00:00\n",
            "Collecting google-cloud-core<3.0.0,>=2.4.2\n",
            "  Downloading google_cloud_core-2.5.0-py3-none-any.whl (29 kB)\n",
            "Collecting google-auth<3.0.0,>=2.26.1\n",
            "  Downloading google_auth-2.48.0-py3-none-any.whl (236 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.5/236.5 kB 37.2 MB/s eta 0:00:00\n",
            "Collecting google-crc32c<2.0.0,>=1.1.3\n",
            "  Downloading google_crc32c-1.8.0-cp39-cp39-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (33 kB)\n",
            "Collecting googleapis-common-protos<2.0.0,>=1.56.2\n",
            "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.5/297.5 kB 40.3 MB/s eta 0:00:00\n",
            "Collecting proto-plus<2.0.0,>=1.22.3\n",
            "  Downloading proto_plus-1.27.1-py3-none-any.whl (50 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 9.8 MB/s eta 0:00:00\n",
            "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5\n",
            "  Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 323.5/323.5 kB 46.3 MB/s eta 0:00:00\n",
            "Collecting cryptography>=38.0.3\n",
            "  Downloading cryptography-46.0.5-cp38-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 92.1 MB/s eta 0:00:00\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 32.5 MB/s eta 0:00:00\n",
            "Collecting charset_normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.4.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.0/154.0 kB 30.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage~=3.9.0->-r requirements.txt (line 4)) (2025.10.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage~=3.9.0->-r requirements.txt (line 4)) (2.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage~=3.9.0->-r requirements.txt (line 4)) (3.11)\n",
            "Requirement already satisfied: typing-extensions>=4.13.2 in /usr/local/lib/python3.9/site-packages (from cryptography>=38.0.3->google-auth<3.0.0,>=2.26.1->google-cloud-storage~=3.9.0->-r requirements.txt (line 4)) (4.15.0)\n",
            "Collecting cffi>=2.0.0\n",
            "  Downloading cffi-2.0.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 216.5/216.5 kB 32.7 MB/s eta 0:00:00\n",
            "Collecting pyasn1<0.7.0,>=0.6.1\n",
            "  Downloading pyasn1-0.6.2-py3-none-any.whl (83 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.4/83.4 kB 17.1 MB/s eta 0:00:00\n",
            "Collecting pycparser\n",
            "  Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.1/118.1 kB 22.9 MB/s eta 0:00:00\n",
            "Installing collected packages: threadpoolctl, pycparser, pyasn1, protobuf, numpy, joblib, google-crc32c, charset_normalizer, scipy, rsa, requests, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, cffi, scikit-learn, cryptography, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
            "Successfully installed cffi-2.0.0 charset_normalizer-3.4.4 cryptography-46.0.5 google-api-core-2.29.0 google-auth-2.48.0 google-cloud-core-2.5.0 google-cloud-storage-3.9.0 google-crc32c-1.8.0 google-resumable-media-2.8.0 googleapis-common-protos-1.72.0 joblib-1.5.3 numpy-1.26.4 proto-plus-1.27.1 protobuf-6.33.5 pyasn1-0.6.2 pyasn1-modules-0.4.2 pycparser-2.23 requests-2.32.5 rsa-4.9.1 scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.6.0\n",
            "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "\u001b[0m\u001b[91m\n",
            "[notice] A new release of pip is available: 23.0.1 -> 26.0.1\n",
            "[notice] To update, run: pip install --upgrade pip\n",
            "\u001b[0mRemoving intermediate container fc35702771e1\n",
            " ---> 6defa0c5076a\n",
            "Successfully built 6defa0c5076a\n",
            "Successfully tagged europe-west4-docker.pkg.dev/notebooks-platform-rd/my-docker-repo-unique/sklearn-fastapi-server:latest\n",
            "PUSH\n",
            "Pushing europe-west4-docker.pkg.dev/notebooks-platform-rd/my-docker-repo-unique/sklearn-fastapi-server\n",
            "The push refers to repository [europe-west4-docker.pkg.dev/notebooks-platform-rd/my-docker-repo-unique/sklearn-fastapi-server]\n",
            "50baaca69be0: Preparing\n",
            "8ed1a345bf01: Preparing\n",
            "81734c9af0fa: Preparing\n",
            "3a2c32313842: Preparing\n",
            "e5ce217468ed: Preparing\n",
            "d57655fa6a90: Preparing\n",
            "5f70bf18a086: Preparing\n",
            "41d0f800a4c3: Preparing\n",
            "881ce2fed861: Preparing\n",
            "c4c8bdf929b6: Preparing\n",
            "ff09a7fa0806: Preparing\n",
            "503e4cc3953e: Preparing\n",
            "ee2b6cee8c49: Preparing\n",
            "7d4c3539da78: Preparing\n",
            "0a684813ab7d: Preparing\n",
            "4e92062d58c4: Preparing\n",
            "05c88577d350: Preparing\n",
            "0b38a99061e1: Preparing\n",
            "713ad1f34d94: Preparing\n",
            "c041ff8d6c73: Preparing\n",
            "fbde375eafc7: Preparing\n",
            "f2522c6ed78b: Preparing\n",
            "ff09a7fa0806: Waiting\n",
            "503e4cc3953e: Waiting\n",
            "ee2b6cee8c49: Waiting\n",
            "7d4c3539da78: Waiting\n",
            "0a684813ab7d: Waiting\n",
            "4e92062d58c4: Waiting\n",
            "05c88577d350: Waiting\n",
            "0b38a99061e1: Waiting\n",
            "713ad1f34d94: Waiting\n",
            "c041ff8d6c73: Waiting\n",
            "fbde375eafc7: Waiting\n",
            "f2522c6ed78b: Waiting\n",
            "41d0f800a4c3: Layer already exists\n",
            "e5ce217468ed: Layer already exists\n",
            "d57655fa6a90: Layer already exists\n",
            "881ce2fed861: Layer already exists\n",
            "5f70bf18a086: Layer already exists\n",
            "3a2c32313842: Layer already exists\n",
            "c4c8bdf929b6: Layer already exists\n",
            "ff09a7fa0806: Layer already exists\n",
            "503e4cc3953e: Layer already exists\n",
            "05c88577d350: Layer already exists\n",
            "7d4c3539da78: Layer already exists\n",
            "4e92062d58c4: Layer already exists\n",
            "0b38a99061e1: Layer already exists\n",
            "ee2b6cee8c49: Layer already exists\n",
            "0a684813ab7d: Layer already exists\n",
            "713ad1f34d94: Layer already exists\n",
            "c041ff8d6c73: Layer already exists\n",
            "fbde375eafc7: Layer already exists\n",
            "81734c9af0fa: Pushed\n",
            "8ed1a345bf01: Pushed\n",
            "f2522c6ed78b: Pushed\n",
            "50baaca69be0: Pushed\n",
            "latest: digest: sha256:2e981f7d91a9bfbb2c4290933140ba15148fc02f398bfd334c59dfacd146ad34 size: 4918\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                                                                                    STATUS\n",
            "cdf2a85b-7435-4751-a788-d9ee3c1efb56  2026-02-13T15:52:10+00:00  1M12S     gs://notebooks-platform-rd_cloudbuild/source/1770997916.365068-bed6df7c093b497db5c893d73520d48c.tgz  europe-west4-docker.pkg.dev/notebooks-platform-rd/my-docker-repo-unique/sklearn-fastapi-server (+1 more)  SUCCESS\n"
          ]
        }
      ],
      "source": [
        "!ls -a\n",
        "!cat .dockerignore\n",
        "!gcloud --project={PROJECT_ID} builds submit --region={LOCATION} --tag={LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b438bfa2129f"
      },
      "source": [
        "## Deploy to Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ae19df6a33e"
      },
      "source": [
        "### Create Vertex AI model using artifact uri\n",
        "Use the Python SDK to upload your model artifact in Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2738154345d5"
      },
      "outputs": [],
      "source": [
        "model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_DISPLAY_NAME,\n",
        "    artifact_uri=f\"{BUCKET_URI}/{MODEL_ARTIFACT_DIR}\",\n",
        "    serving_container_image_uri=f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1b85afc7df"
      },
      "source": [
        "### Deploy the model to Vertex AI Endpoint\n",
        "\n",
        "Once the deployment process gets done, the model is deployed and ready for serving online predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "62cf66498a28"
      },
      "outputs": [],
      "source": [
        "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6883e7b07143"
      },
      "source": [
        "## Request predictions\n",
        "\n",
        "Send online requests to the endpoint and get predictions.\n",
        "\n",
        "### Using Python SDK\n",
        "\n",
        "Get predictions from the endpoint for a sample input using python SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69ed411c2d3"
      },
      "outputs": [],
      "source": [
        "# Send some sample data to the endpoint\n",
        "endpoint.predict(instances=[[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "370d22f53427"
      },
      "source": [
        "### Using REST\n",
        "\n",
        "Get predictions from the endpoint using curl request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba55bc560d58"
      },
      "outputs": [],
      "source": [
        "# Fetch the endpoint name\n",
        "ENDPOINT_ID = endpoint.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95c562b4e98b"
      },
      "outputs": [],
      "source": [
        "# Send a prediction request using sample data\n",
        "! curl \\\n",
        "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d @instances.json \\\n",
        "https://{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}:predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa71174a7dd0"
      },
      "source": [
        "### Using gcloud CLI\n",
        "\n",
        "Get predictions from the endpoint using gcloud CLI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23b8e807b02c"
      },
      "outputs": [],
      "source": [
        "!gcloud ai endpoints predict $ENDPOINT_ID \\\n",
        "  --region=$LOCATION \\\n",
        "  --json-request=instances.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Model\n",
        "- Endpoint\n",
        "- Artifact Registry Image\n",
        "- Artifact Repository: Set `delete_art_repo` to **True** to delete the repository created in this tutorial.\n",
        "- Cloud Storage bucket: Set `delete_bucket` to **True** to delete the Cloud Storage bucket used in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "delete_art_repo = False\n",
        "\n",
        "# Undeploy model and delete endpoint\n",
        "endpoint.undeploy_all()\n",
        "endpoint.delete()\n",
        "\n",
        "#Delete the model resource\n",
        "model.delete()\n",
        "\n",
        "# Delete the container image from Artifact Registry\n",
        "!gcloud artifacts docker images delete \\\n",
        "    --quiet \\\n",
        "    --delete-tags \\\n",
        "    {LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\n",
        "\n",
        "# Delete the Artifact Repository\n",
        "if delete_art_repo:\n",
        "    ! gcloud artifacts repositories delete {REPOSITORY} --location=$LOCATION -q\n",
        "\n",
        "# Delete the Cloud Storage bucket\n",
        "if delete_bucket:\n",
        "    ! gcloud storage rm --recursive $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31885b85cc2c"
      },
      "source": [
        "Clean up the locally created files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "234ea94b87c5"
      },
      "outputs": [],
      "source": [
        "! rm -rf app/\n",
        "! rm requirements.txt\n",
        "! rm instances.json\n",
        "! rm Dockerfile"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "SDK_Custom_Container_Prediction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}